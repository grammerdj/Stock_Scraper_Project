{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fead7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook exists to compile stock data that machine learning can run off of, and save it to a .json file\n",
    "#for each stock. The rough idea of all of the data that the scraper will collect is as follows:\n",
    "#[Date, Open, High, Low, Close, Adj Close, Volume, State of the Market (bear, bull, normal), \n",
    "#State of the Economy (Dow Jones, S&P500, NASDAQ Composite), inflation %, \n",
    "#whether or not the company is releasing earnings in the next day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8757c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebook instructions\n",
    "\n",
    "#The purpose of this scraper is to collect data that can be fed into ML algorithms. If your ML algorithm is working\n",
    "#correctly, this program can run overnight, every night (It finishes within that 8 hour window. If you aren't getting\n",
    "#8 hours of sleep per night, consider it please) to collect updated data. The update functions are if you are trying to\n",
    "#update State of the Market (bear, bull, normal), inflation %, or earnings data separately from the historical price \n",
    "#date. If you are not trying to do that, running the whole notebook every day will automatically update those metrics\n",
    "#when they are updated by their respective agencies.\n",
    "\n",
    "#The recommended paths I use for this notebook are just recommended. I saved everything within the Jupyter notebook \n",
    "#file, but you do not have to do that. Just make sure you update the paths.\n",
    "\n",
    "#This scraper returns about 7.5 GB of data.\n",
    "\n",
    "#There are certain stocks that fail to scrape every time. The failed scrapes-related functions exist to compare whether\n",
    "#or not the same functions are failing to scrape every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff7f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains all necessary import statements\n",
    "import math\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf \n",
    "plt.style.use('fivethirtyeight')\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed06220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell defines the functions neccessary to read and write JSON files\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def write_json(path, data):\n",
    "    with open(path, 'w', encoding = \"utf-8\") as f:\n",
    "        json.dump(data, f, indent = 2)\n",
    "def read_json(path):\n",
    "    with open(path, encoding = \"utf-8\") as f: \n",
    "        return json.load(f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ba3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell defines a function that will continuously check your internet connection after every stock scrape, ensuring\n",
    "#if you leave the scraper on as a background process it will not fail to scrape stocks due to loss of connectivity\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def check_wifi():\n",
    "    IPaddress=socket.gethostbyname(socket.gethostname())\n",
    "    if IPaddress==\"127.0.0.1\":\n",
    "        print('Oops, no internet. Please try again')\n",
    "        time.sleep(20)\n",
    "        check_wifi()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869a3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will retrive the yyyy-mm-dd formatted date of any MONTH/D style date\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def get_date(investor_sentiment_reported_date):\n",
    "    if investor_sentiment_reported_date[0:3] == 'Jan':\n",
    "        month = '01'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Feb':\n",
    "        month = '02'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Mar':\n",
    "        month = '03'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Apr':\n",
    "        month = '04'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'May':\n",
    "        month = '05'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Jun':\n",
    "        month = '06'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Jul':\n",
    "        month = '07'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Aug':\n",
    "        month = '08'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Sep':\n",
    "        month = '09'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Oct':\n",
    "        month = '10'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Nov':\n",
    "        month = '11'\n",
    "    elif investor_sentiment_reported_date[0:3] == 'Dec':\n",
    "        month = '12'\n",
    "    day = '0'+investor_sentiment_reported_date[4]\n",
    "    year = datetime.date.today().strftime(\"%Y-%m-%d\")[0:4]\n",
    "    reported_date = year + '-'+ month + '-' + day\n",
    "    return reported_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade82e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a custom business day calendar that excludes days the market isn't open\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def custom_business(start, end):\n",
    "    start = start\n",
    "    end = (datetime.date(int(end[0:4]), int(end[5:7]), int(end[8:10])) + datetime.timedelta(weeks=1)).strftime(\"%Y-%m-%d\")\n",
    "    start_list = start.split('-')\n",
    "    for idx in range(0, len(start_list)):\n",
    "        if idx == 0:\n",
    "            start_year = int(start_list[idx])\n",
    "        elif idx == 1:\n",
    "            start_month = int(start_list[idx])\n",
    "        else:\n",
    "            start_day  = int(start_list[idx])\n",
    "    end_list = end.split('-')\n",
    "    for idx in range(0, len(start_list)):\n",
    "        if idx == 0:\n",
    "            end_year = int(end_list[idx])\n",
    "        elif idx == 1:\n",
    "            end_month = int(end_list[idx])\n",
    "        else:\n",
    "            end_day  = int(end_list[idx])\n",
    "    holidays = [datetime.datetime(start_year, start_month, start_day), datetime.datetime(end_year, end_month, end_day)]\n",
    "    dates = pd.bdate_range(start=start, end=end, freq = 'C', holidays = holidays).strftime(\"%Y-%m-%d\").tolist()\n",
    "    for idx in range(0, len(dates)-2):\n",
    "        if dates[idx] == '2004-12-31' or \\\n",
    "        dates[idx] == '2006-01-02' or\\\n",
    "        dates[idx] == '2010-12-31' or\\\n",
    "        dates[idx] == '2012-01-02' or\\\n",
    "        dates[idx] == '2017-01-02' or\\\n",
    "        dates[idx] == '2021-12-31' or\\\n",
    "        dates[idx] == '2023-01-02':\n",
    "            dates.pop(idx)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f67119e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell turns the scraped data into a DataFrame\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def get_data_frame(rows, columns):\n",
    "    stocks_df = pd.DataFrame(columns=columns)\n",
    "    for row in rows:\n",
    "        elems = row.find_all('td')\n",
    "        dict_to_add = {}\n",
    "        for i,elem in enumerate(elems):\n",
    "            dict_to_add[columns[i]] = elem.text\n",
    "        stocks_df = stocks_df.append(dict_to_add, ignore_index=True)\n",
    "    stocks_df_symbols = stocks_df['Symbol']\n",
    "    stocks_df_symbols_list = list(stocks_df_symbols)\n",
    "    return stocks_df_symbols_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9338eda6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This function will compile a list of every publicly traded stock on the market, save it to a JSON, and also return the\n",
    "#   sorted list. This function uses the website EODDATA as a source.\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def make_stock_tuple_list(): \n",
    "    parsed_html_dict = {}\n",
    "    parsed_html_dict_nas = {}\n",
    "    url = 'https://eoddata.com/stocklist/NYSE/{}.htm'\n",
    "    url_nas = 'https://eoddata.com/stocklist/NASDAQ/{}.htm'\n",
    "    #This loop will loop through the pages of EOD Data to get all of the NYSE stock symbols\n",
    "    for letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \\\n",
    "                   'U', 'V', 'W', 'X', 'Y', 'Z']:\n",
    "        page_url = url.format(letter)\n",
    "        page = requests.get(page_url)\n",
    "        assert page.status_code == 200\n",
    "        data = BeautifulSoup(page.text, 'html.parser')\n",
    "        letter_table_data = data.find('table', {'class': 'quotes'})\n",
    "        assert len(letter_table_data) > 0\n",
    "        table_header = letter_table_data.find_all('th')\n",
    "        text_headers = []\n",
    "        for th in table_header:\n",
    "            th_text = th.text\n",
    "            text_headers.append(th_text)\n",
    "        assert len(text_headers) == 8\n",
    "        text_headers.pop(-1)\n",
    "        table_data = letter_table_data.find_all('tr')\n",
    "        table_data.pop(0)\n",
    "        stock_tuple_list = []\n",
    "        for row in table_data:\n",
    "            stock_symbol = row.find_all('td')\n",
    "            for td in stock_symbol:\n",
    "                if stock_symbol.index(td)==text_headers.index('Code'):\n",
    "                    symbol = td.text\n",
    "                if stock_symbol.index(td)==text_headers.index('Name'):\n",
    "                    name = td.text\n",
    "            stock_tuple = (symbol, name)\n",
    "            stock_tuple_list.append(stock_tuple)\n",
    "        parsed_html_dict[letter] = stock_tuple_list\n",
    "    #This loop will loop through the pages of EOD Data to get all of the NASDAQ Stock Symbols\n",
    "        page_url_nas = url_nas.format(letter)\n",
    "        page_nas = requests.get(page_url_nas)\n",
    "        assert page_nas.status_code == 200\n",
    "        data_nas = BeautifulSoup(page_nas.text, 'html.parser')\n",
    "        letter_table_data_nas = data_nas.find('table', {'class': 'quotes'})\n",
    "        assert len(letter_table_data_nas) > 0\n",
    "        table_header_nas = letter_table_data_nas.find_all('th')\n",
    "        text_headers_nas = []\n",
    "        for th_nas in table_header_nas:\n",
    "            th_text_nas = th_nas.text\n",
    "            text_headers_nas.append(th_text_nas)\n",
    "        assert len(text_headers_nas) == 8\n",
    "        text_headers_nas.pop(-1)\n",
    "        table_data_nas = letter_table_data_nas.find_all('tr')\n",
    "        table_data_nas.pop(0)\n",
    "        stock_tuple_list_nas = []\n",
    "        for row_nas in table_data_nas:\n",
    "            stock_symbol_nas = row_nas.find_all('td')\n",
    "            for td_nas in stock_symbol_nas:\n",
    "                if stock_symbol_nas.index(td_nas)==text_headers_nas.index('Code'):\n",
    "                    symbol_nas = td_nas.text\n",
    "                if stock_symbol_nas.index(td_nas)==text_headers_nas.index('Name'):\n",
    "                    name_nas = td_nas.text\n",
    "            stock_tuple_nas = (symbol_nas, name_nas)\n",
    "            stock_tuple_list_nas.append(stock_tuple_nas)\n",
    "        parsed_html_dict_nas[letter] = stock_tuple_list_nas\n",
    "        time.sleep(20)\n",
    "    #This code will take both parsed_html_dicts and add them to create a dictionary of all stock symbols traded on both \n",
    "    #   platforms\n",
    "    complete_stock_symbol_list = []\n",
    "    for key in parsed_html_dict:\n",
    "        nyse_list = parsed_html_dict[key]\n",
    "        nas_list = parsed_html_dict_nas[key]\n",
    "        total_list = list(set(nyse_list + nas_list))\n",
    "        complete_stock_symbol_list.extend(total_list)\n",
    "    complete_stock_symbol_list.sort()\n",
    "    print(len(complete_stock_symbol_list))\n",
    "    #This cell will write the contents of complete_stock_symbol_list to a .json file\n",
    "    path_name = os.path.join('List_of_Stocks', 'nyse_and_nasdaq_stock_symbol_list.json')\n",
    "    write_json(path_name, complete_stock_symbol_list)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad97e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will pull the complete stock tuple list from the JSON format.\n",
    "#Recommended Path is os.path.join('List_of_Stocks', 'nyse_and_nasdaq_stock_symbol_list.json')\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_stock_tuple_list(path):\n",
    "    stock_list_of_lists = read_json(path)\n",
    "    return stock_list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89102274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will scrape the historical yahoo stock data for a given stock tuple\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_historical_pricing(stock_tuple):\n",
    "    yf.pdr_override()\n",
    "    df = web.get_data_yahoo(stock_tuple[0])\n",
    "    if df.shape[0] == 0:\n",
    "        df = web.get_data_yahoo(stock_tuple[0], start='2004-01-01')\n",
    "        if df.shape[0] == 0:\n",
    "            time.sleep(0.2)\n",
    "            return stock_tuple\n",
    "        else:\n",
    "            time.sleep(0.2)\n",
    "            string_index_list = []\n",
    "            for index in df.index:\n",
    "                string_index = str(index)[0:10]\n",
    "                string_index_list.append(string_index)\n",
    "            df['string_index'] = string_index_list\n",
    "            df = df.set_index('string_index')\n",
    "            return df\n",
    "    else:\n",
    "        time.sleep(0.2)\n",
    "        string_index_list = []\n",
    "        for index in df.index:\n",
    "            string_index = str(index)[0:10]\n",
    "            string_index_list.append(string_index)\n",
    "        df['string_index'] = string_index_list\n",
    "        df = df.set_index('string_index')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830b9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will retrive the investor sentiment data, while mapping it ahead to the current date.\n",
    "#Recommended Path is os.path.join('Market_Trend_Data', 'Bullish_Bearish_Neutral_Data')\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_investor_sentiment(bull_path):\n",
    "    json_string = read_json(bull_path)\n",
    "    bull_df = pd.read_json(json_string, orient = 'columns')\n",
    "    string_index_list = []\n",
    "    for index in bull_df.index:\n",
    "        string_index = index.strftime(\"%Y-%m-%d\")\n",
    "        string_index_list.append(string_index)\n",
    "    bull_df['string_index'] = string_index_list\n",
    "    bull_df = bull_df.set_index('string_index')\n",
    "    current_date = datetime.date.today()\n",
    "    last_reported_date = bull_df.tail(1).index[0]\n",
    "    date_list = pd.date_range(last_reported_date, current_date, freq = 'B').strftime(\"%Y-%m-%d\").tolist()\n",
    "    date_list.pop(0)\n",
    "    n = len(date_list)\n",
    "    if len(date_list) == 0:\n",
    "        return bull_df\n",
    "    else:\n",
    "        supplemental_data_dictionary = {'Date': date_list, 'Bullish': [bull_df.loc[bull_df.tail(1).index[0]][0]]*n, \\\n",
    "                                        'Neutral': [bull_df.loc[bull_df.tail(1).index[0]][1]]*n,\\\n",
    "                       'Bearish': [bull_df.loc[bull_df.tail(1).index[0]][2]]*n}\n",
    "        supplemental_data_frame = pd.DataFrame(supplemental_data_dictionary)\n",
    "        supplemental_data_frame = supplemental_data_frame.set_index('Date')\n",
    "        bull_df = pd.concat([bull_df, supplemental_data_frame], axis = 0)\n",
    "        return bull_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a797f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will retrieve the inflation % data, while mapping it ahead to the current date\n",
    "#Recommended Path is os.path.join('Market_Trend_Data', 'Inflation_Percentage_History')\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_inflation(inf_path):\n",
    "    inf_json_string = read_json(inf_path)\n",
    "    inf_df = pd.read_json(inf_json_string, orient='columns')\n",
    "    string_index_list = []\n",
    "    for index in inf_df.index:\n",
    "        string_index = index.strftime(\"%Y-%m-%d\")\n",
    "        string_index_list.append(string_index)\n",
    "    inf_df['string_index'] = string_index_list\n",
    "    inf_df = inf_df.set_index('string_index')\n",
    "    current_date = datetime.date.today()\n",
    "    last_reported_date_inf = inf_df.tail(1).index[0]\n",
    "    date_list_inf = pd.date_range(last_reported_date_inf, current_date, freq = 'B').strftime(\"%Y-%m-%d\").tolist()\n",
    "    if date_list_inf[0] == last_reported_date_inf:\n",
    "        date_list_inf.pop(0)\n",
    "    if len(date_list_inf) == 0:\n",
    "        return inf_df\n",
    "    else:\n",
    "        n = len(date_list_inf)\n",
    "        supplemental_data_dictionary_inf = {'string_index': date_list_inf, 'Inflation %': [inf_df.loc[inf_df.tail(1).index[0]][0]]*n}\n",
    "        supplemental_data_frame_inf = pd.DataFrame(supplemental_data_dictionary_inf)\n",
    "        supplemental_data_frame_inf = supplemental_data_frame_inf.set_index('string_index')\n",
    "        inf_df = pd.concat([inf_df, supplemental_data_frame_inf], axis = 0)\n",
    "        return inf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66713981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions will retrieve the yahoo data for the DOW, S&P500, and NASDAQ COMPOSITE indices\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_dow():\n",
    "    yf.pdr_override()\n",
    "    df_dow = web.get_data_yahoo('^DJI')\n",
    "    df2_dow = pd.DataFrame(df_dow['Close'])\n",
    "    df2_dow.rename(columns ={'Close': 'Dow_Close'}, inplace = True)\n",
    "    string_index_list = []\n",
    "    for index in df2_dow.index:\n",
    "        string_index = str(index)[0:10]\n",
    "        string_index_list.append(string_index)\n",
    "    df2_dow['string_index'] = string_index_list\n",
    "    df2_dow = df2_dow.set_index('string_index')\n",
    "    time.sleep(0.2)\n",
    "    return df2_dow\n",
    "def obtain_sp():\n",
    "    yf.pdr_override()\n",
    "    df_sp500 = web.get_data_yahoo('^GSPC')\n",
    "    df2_sp500 = pd.DataFrame(df_sp500['Close'])\n",
    "    df2_sp500.rename(columns ={'Close': 'SP500_Close'}, inplace = True)\n",
    "    string_index_list = []\n",
    "    for index in df2_sp500.index:\n",
    "        string_index = str(index)[0:10]\n",
    "        string_index_list.append(string_index)\n",
    "    df2_sp500['string_index'] = string_index_list\n",
    "    df2_sp500 = df2_sp500.set_index('string_index')\n",
    "    time.sleep(0.2)\n",
    "    return df2_sp500\n",
    "def obtain_nasdaq():\n",
    "    yf.pdr_override()\n",
    "    df_nasdaq_comp = web.get_data_yahoo('^IXIC')\n",
    "    df2_nasdaq_comp = pd.DataFrame(df_nasdaq_comp['Close'])\n",
    "    df2_nasdaq_comp.rename(columns ={'Close': 'NASDAQ_Close'}, inplace = True)\n",
    "    string_index_list = []\n",
    "    for index in df2_nasdaq_comp.index:\n",
    "        string_index = str(index)[0:10]\n",
    "        string_index_list.append(string_index)\n",
    "    df2_nasdaq_comp['string_index'] = string_index_list\n",
    "    df2_nasdaq_comp = df2_nasdaq_comp.set_index('string_index')\n",
    "    time.sleep(0.2)\n",
    "    return df2_nasdaq_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a05a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will obtain earnings data from the JSON format\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def obtain_earnings(earnings_path):\n",
    "    earnings_dict = read_json(earnings_path)\n",
    "    return earnings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4c7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains the function that will map the earnings reports onto the historical stock jsons, having values \n",
    "#  of either 0 (false) or 1 (true)\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def map_earnings(earnings_dict, stock_df, stock_symbol):\n",
    "    boolean_dictionary = {}\n",
    "    for int_pos in range(0, len(stock_df)):\n",
    "        index = str(stock_df.index[int_pos])[0:10]\n",
    "        if index in boolean_dictionary:\n",
    "            continue\n",
    "        elif index not in earnings_dict:\n",
    "            boolean_dictionary[index] = 0\n",
    "        elif stock_symbol in earnings_dict[index]:\n",
    "            boolean_dictionary[index] = 0\n",
    "            boolean_dictionary[str(stock_df.index[int_pos-1])[0:10]] = 1\n",
    "        else:\n",
    "            boolean_dictionary[index] = 0\n",
    "    boolean_df = pd.DataFrame.from_dict(boolean_dictionary, orient='index', columns = ['Releasing Earnings Tomorrow'])\n",
    "    return boolean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3cf9e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will save the failed stock symbols into a json for later comparison\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def save_failed_scrapes(failed_stock_retrievals):\n",
    "    time_name_sort = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "    last_path_name = 'failed_stock_symbol_list_' + time_name_sort\n",
    "    failed_path = os.path.join('List_of_Stocks', last_path_name)\n",
    "    write_json(failed_path, failed_stock_retrievals)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86553f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will merge all of the dataframes that have been created into one master dataframe\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def merge_dfs(stock_df, market_trend_data_df, inflation_data_df, dow_df, sp_df, nasdaq_df, earnings_df):\n",
    "    df2 = pd.merge(stock_df, market_trend_data_df, left_index=True, right_index=True)\n",
    "    df3 = pd.merge(df2, inflation_data_df, left_index=True, right_index=True)\n",
    "    df4 = pd.merge(df3, dow_df, left_index=True, right_index=True)\n",
    "    df5 = pd.merge(df4, sp_df, left_index=True, right_index=True)\n",
    "    df6 = pd.merge(df5, nasdaq_df, left_index=True, right_index=True)\n",
    "    final_df = pd.merge(df6, earnings_df, left_index = True, right_index = True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c424bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will save the final master dataframe of a stock to its proper json\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def master_json(final_df, stock_tuple):\n",
    "    df_json = final_df.to_json(orient = 'columns')\n",
    "    symbol_path_name = stock_tuple[0]+'_final_data.json'\n",
    "    path_name = os.path.join('Stock_Jsons', symbol_path_name)\n",
    "    write_json(path_name, df_json)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08671824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will compare the failed stock retrievals of all of the scrapes\n",
    "#THIS FUNCTION HAS NOT BEEN DEBUGGED\n",
    "def failed_scrape_analysis():\n",
    "    path = 'List_of_Stocks'\n",
    "    dir_list = os.listdir(path)\n",
    "    failed_list = []\n",
    "    dictionary_failed_stocks = {}\n",
    "    for file in dir_list:\n",
    "        if file[0:6] == 'failed':\n",
    "            failed_list.append(file)\n",
    "            f = read_json(os.path.join(path, file))\n",
    "            dictionary_failed_stocks[file] = f\n",
    "        else:\n",
    "            continue\n",
    "    failed_list.sort()\n",
    "    initial_failed_stocks = dictionary_failed_stocks[failed_list[0]]\n",
    "    failed_stock_info = {}\n",
    "    for name in failed_list:\n",
    "        failed_stock_list = dictionary_failed_stocks[name]\n",
    "        key = name \n",
    "        length = len(failed_stock_list)\n",
    "        if failed_list.index(name) != 0:\n",
    "            exclusive_failed_stocks = []\n",
    "            for stock in failed_stock_list:\n",
    "                if stock not in initial_failed_stocks:\n",
    "                    exclusive_failed_stocks.append(stock)\n",
    "            failed_stock_info[(key, length)] = exclusive_failed_stocks\n",
    "        else:\n",
    "            failed_stock_info[(key, length)] = 'No Exclusives for First Failed Scrapes'\n",
    "    return failed_stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ca9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will update investor sentiment data when released \n",
    "#This cell should be run every thursday\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def update_sentiment_data(bull_path):\n",
    "    url = 'https://www.aaii.com/sentimentsurvey/sent_results'\n",
    "    headers = {'authority': 'www.aaii.com',\\\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\\\n",
    "               'accept-language': 'en-US,en;q=0.9', \\\n",
    "               'referer': 'https://www.aaii.com/sentimentsurvey/sent_results', \\\n",
    "               'DNT': '1',\\\n",
    "                'sec-ch-ua-mobile': '?0',\\\n",
    "                'sec-ch-ua-platform': '\"Windows\"',\\\n",
    "                'sec-fetch-dest': 'style',\\\n",
    "                'sec-fetch-mode': 'no-cors',\\\n",
    "                'sec-fetch-site': 'same-origin',\\\n",
    "              'Clear-Site-Data': '\"cache\", \"cookies\", \"storage\"', \\\n",
    "              'cookie': 'CFID=23072366; CFTOKEN=2d012472349cc840-670874C0-0C4D-829C-F16D771B042838EE; JSESSIONID=5B858A89E32DC8109B3C7D6BD096D587.cfusion; HASLOGGEDIN=\"\"; nlbi_1875454=Imi+eMas7C+8eZxO5D1AxQAAAACpFtuMdTiO2aODVUW3LBx/; visid_incap_1875454=SGqJ7WPmTQeFxRqZj5ywwuDouGMAAAAAQUIPAAAAAACVIyIktevmwmwDks+02l1+; incap_ses_617_1875454=CMJLK3ymw0bE8Sx7wgaQCOHouGMAAAAAU/XbeMfpYNqUCN+VUsaWNg==; _ga_M82LSVK8P9=GS1.1.1673062625.7.0.1673062625.0.0.0'}\n",
    "    page = requests.get(url, headers = headers)\n",
    "    assert page.status_code == 200\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    headers = table.find_all('tr')[0]\n",
    "    headers_list = []\n",
    "    for header in headers:\n",
    "        header_title = header.text\n",
    "        if header_title == '\\n':\n",
    "            continue\n",
    "        else:\n",
    "            headers_list.append(header_title)\n",
    "    columns = headers_list\n",
    "    print(columns)\n",
    "    rows = table.find_all('tr')[1:]\n",
    "    new_df = pd.DataFrame(columns=columns)\n",
    "    for row in rows:\n",
    "        elems = row.find_all('td')\n",
    "        dict_to_add = {}\n",
    "        for i,elem in enumerate(elems):\n",
    "            dict_to_add[columns[i]] = elem.text\n",
    "        new_df = new_df.append(dict_to_add, ignore_index=True)\n",
    "    json_string = read_json(bull_path)\n",
    "    bull_df = pd.read_json(json_string, orient = 'columns')\n",
    "    string_index_list = []\n",
    "    for index in bull_df.index:\n",
    "        string_index = index.strftime(\"%Y-%m-%d\")\n",
    "        string_index_list.append(string_index)\n",
    "    bull_df['string_index'] = string_index_list\n",
    "    bull_df = bull_df.set_index('string_index')\n",
    "    if bull_df.tail(1)['Bullish'][0] == new_df.head(1)['Bullish'][0][0:4]:\n",
    "        print('THE INVESTOR SENTIMENT DATA IS ALREADY UPDATED')\n",
    "        return None\n",
    "    else:\n",
    "        latest_reported_date = get_date(new_df.head(1)['Reported Date'][0])\n",
    "        second_latest_reported_date = ((datetime.date(int(latest_reported_date[0:4]), \\\n",
    "            int(latest_reported_date[5:7]), int(latest_reported_date[8:10])))\\\n",
    "                               - datetime.timedelta(weeks=1)).strftime(\"%Y-%m-%d\")\n",
    "        date_list = pd.date_range(second_latest_reported_date, latest_reported_date, freq = 'B').strftime(\"%Y-%m-%d\").tolist()\n",
    "        date_list.pop(0)\n",
    "        bull_bullish = float(new_df.head(1)['Bullish'][0][0:4])/100\n",
    "        bull_neutral = float(new_df.head(1)['Neutral'][0][0:4])/100\n",
    "        bull_bearish = float(new_df.head(1)['Bearish'][0][0:4])/100\n",
    "        bull_dictionary = {'Date': date_list, 'Bullish': bull_bullish, 'Neutral': bull_neutral,\\\n",
    "                   'Bearish': bull_bearish}\n",
    "        bull_df = bull_df.loc[bull_df.head(1).index[0]:second_latest_reported_date]\n",
    "        new_bull_df = pd.DataFrame(bull_dictionary)\n",
    "        new_bull_df = new_bull_df.set_index('Date')\n",
    "        appended_bull_df = pd.concat([bull_df, new_bull_df], axis = 0) \n",
    "        bull_json_string = appended_bull_df.to_json(orient = 'columns')\n",
    "        write_json(bull_path, bull_json_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a4af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will update the inflation data with the current months inflation statistics\n",
    "#This cell should be run every 12th/13th of every month\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def update_inflation_data(inf_path):\n",
    "    start_date = str(input('Please Input Start Date of Inflation Data yyyy-mm-dd'))\n",
    "    end_date = str(input('Please Input End Date of Inflation Data yyyy-mm-dd'))\n",
    "    inflation_percent = float(input('Please input inflation % 0.0'))\n",
    "    date_list = pd.date_range(start_date, end_date, freq = 'B').strftime(\"%Y-%m-%d\").tolist()\n",
    "    last_entry_date = (datetime.date(int(start_date[0:4]), int(start_date[5:7]), int(start_date[8:10]))\\\n",
    "                       - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "    n = len(date_list)\n",
    "    supplemental_data_dictionary_inf = {'string_index': date_list, 'Inflation %': [inflation_percent]*n}\n",
    "    supplemental_data_frame_inf = pd.DataFrame(supplemental_data_dictionary_inf)\n",
    "    supplemental_data_frame_inf = supplemental_data_frame_inf.set_index('string_index')\n",
    "    f = read_json(inf_path)\n",
    "    inf_df = pd.read_json(f, orient='columns')\n",
    "    string_index_list = []\n",
    "    for index in inf_df.index:\n",
    "        string_index = index.strftime(\"%Y-%m-%d\")\n",
    "        string_index_list.append(string_index)\n",
    "    inf_df['string_index'] = string_index_list\n",
    "    inf_df = inf_df.set_index('string_index')\n",
    "    inf_df = inf_df.loc[inf_df.head(1).index[0]:last_entry_date]\n",
    "    appended_inf_df = pd.concat([inf_df, supplemental_data_frame_inf], axis = 0)\n",
    "    print(inf_df)\n",
    "    json_string = appended_inf_df.to_json(orient='columns')\n",
    "    write_json(inf_path, json_string)\n",
    "    \n",
    "    \n",
    "    \n",
    "#This code is available in the future if I have too much time on my hands and want to automate a process that only \n",
    "#   requires manual input 1/month\n",
    "\n",
    "#     url = 'https://data.bls.gov/timeseries/CUUR0000SA0L1E?output_view=pct_12mths'\n",
    "#     headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\\\n",
    "#               }\n",
    "#     page = requests.get(url, headers=headers)\n",
    "#     assert page.status_code == 200\n",
    "#     soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#     table = soup.find_all('table')[1]\n",
    "#     print(table)\n",
    "#     headers = table.find_all('tr')[0]\n",
    "#     headers_list = []\n",
    "#     for header in headers:\n",
    "#         header_title = header.text\n",
    "#         if header_title == '\\n':\n",
    "#             continue\n",
    "#         else:\n",
    "#             headers_list.append(header_title)\n",
    "#     columns = headers_list\n",
    "#     print(columns)\n",
    "#     rows = table.find_all('tr')[1:]\n",
    "#     new_df = pd.DataFrame(columns=columns)\n",
    "#     for row in rows:\n",
    "#         elems = row.find('th')\n",
    "#         elems.extend(row.find_all('td'))\n",
    "#         print(elems)\n",
    "#         dict_to_add = {}\n",
    "#         for i,elem in enumerate(elems):\n",
    "#             dict_to_add[columns[i]] = elem.text\n",
    "#         new_df = new_df.append(dict_to_add, ignore_index=True)\n",
    "#     return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c7e8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell scrapes data from yahoo earnings calendar and returns a dictionary with date: [companies]\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def scraper_function(date_object, url, headers, dates):\n",
    "    earnings_reports_dict = {}\n",
    "    date_list = date_object.split('-')\n",
    "    year = int(date_list[0])\n",
    "    month = int(date_list[1])\n",
    "    day = int(date_list[2])\n",
    "    today = datetime.date(year, month, day)\n",
    "    start = (today - datetime.timedelta(days=today.weekday()+1))\n",
    "    end = (start + datetime.timedelta(days=6))\n",
    "    start = str(start)\n",
    "    end = str(end)\n",
    "    today_str = str(today)\n",
    "    page_url=url.format(start, end, today_str)\n",
    "    print(page_url)\n",
    "    page = requests.get(page_url, headers=headers)\n",
    "    print(page.status_code)\n",
    "    if page.status_code == 404:\n",
    "        print(date_object)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    time.sleep(1)\n",
    "    if soup.find('table') == None:\n",
    "        return None\n",
    "    table = soup.find('table')\n",
    "    headers = table.find_all('th')\n",
    "    headers_list = []\n",
    "    for header in headers:\n",
    "        header_title = header.text\n",
    "        headers_list.append(header_title)\n",
    "    columns = headers_list\n",
    "    rows = soup.table.tbody.find_all('tr')\n",
    "    stock_df_symbols_list = get_data_frame(rows, columns)\n",
    "    if date_object in dates:\n",
    "        earnings_reports_dict[date_object] = stock_df_symbols_list\n",
    "    else: \n",
    "        tomorrow = (today + datetime.timedelta(days=1))\n",
    "        tomorrow_str = str(tomorrow)\n",
    "        if tomorrow_str in dates:\n",
    "            if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "            else:\n",
    "                earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "        else:\n",
    "            tomorrow = (tomorrow + datetime.timedelta(days=1))\n",
    "            tomorrow_str = str(tomorrow)\n",
    "            if tomorrow_str in dates:\n",
    "                if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                    earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "                else:\n",
    "                    earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "            else:\n",
    "                tomorrow = (tomorrow + datetime.timedelta(days=1))\n",
    "                tomorrow_str = str(tomorrow)\n",
    "                if tomorrow_str in dates:\n",
    "                    if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                        earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "                    else:\n",
    "                        earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "                else:\n",
    "                    tomorrow = (tomorrow + datetime.timedelta(days=1))\n",
    "                    tomorrow_str = str(tomorrow)\n",
    "                    if tomorrow_str in dates:\n",
    "                        if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                            earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "                        else:\n",
    "                            earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "                    else:\n",
    "                        tomorrow = (tomorrow + datetime.timedelta(days=1))\n",
    "                        tomorrow_str = str(tomorrow)\n",
    "                        if tomorrow_str in dates:\n",
    "                            if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                                earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "                            else:\n",
    "                                earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "                        else:\n",
    "                            tomorrow = (tomorrow + datetime.timedelta(days=1))\n",
    "                            tomorrow_str = str(tomorrow)\n",
    "                            if tomorrow_str in dates:\n",
    "                                if earnings_reports_dict.get(tomorrow_str) == None:\n",
    "                                    earnings_reports_dict[tomorrow_str] = stock_df_symbols_list\n",
    "                                else:\n",
    "                                    earnings_reports_dict[tomorrow_str].extend(stock_df_symbols_list)\n",
    "    return earnings_reports_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2a532b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function updates the earnings reports for the week that it is called for \n",
    "#This function should be run every week\n",
    "#ALL FUNCTIONS CONTAINED IN THIS CELL ARE DEBUGGED\n",
    "def update_earnings_reports(start, end, earnings_path):\n",
    "    start = start\n",
    "    end = end\n",
    "    dates = custom_business(start, end)\n",
    "    print(dates)\n",
    "    unsifted_dates = pd.date_range(start = start, end = end, freq ='D').strftime(\"%Y-%m-%d\").tolist()\n",
    "    url = 'https://finance.yahoo.com/calendar/earnings?from={}&to={}&day={}'\n",
    "    headers = {'authority': 's.yimg.com',\\\n",
    "    'method': 'GET',\\\n",
    "    'path': '/uc/finance/dd-site/fonts/YahooSansFinancial-Regular-Web.woff2',\\\n",
    "    'scheme': 'https',\\\n",
    "    'accept': '*/*',\\\n",
    "    'accept-encoding': 'gzip, deflate, br',\\\n",
    "    'accept-language': 'en-US,en;q=0.9',\\\n",
    "    'cache-control': 'no-cache',\\\n",
    "    'origin': 'https://finance.yahoo.com',\\\n",
    "    'pragma': 'no-cache',\\\n",
    "    'referer': 'https://finance.yahoo.com/calendar/earnings?from=2023-12-24&to=2023-12-30&day=2023-12-28',\\\n",
    "    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\\\n",
    "    'sec-ch-ua-mobile': '?0',\\\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\\\n",
    "    'sec-fetch-dest': 'font',\\\n",
    "    'sec-fetch-mode': 'cors',\\\n",
    "    'sec-fetch-site': 'cross-site',\\\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36', \\\n",
    "    'Clear-Site-Data': '\"cache\", \"cookies\", \"storage\"'}\n",
    "    earning_dict = {}\n",
    "    for date_object in unsifted_dates:\n",
    "        additive_dict = scraper_function(date_object, url, headers, dates)\n",
    "        if additive_dict == None:\n",
    "            continue\n",
    "        else:\n",
    "            earning_dict.update(additive_dict)\n",
    "    earnings_historical = read_json(earnings_path)\n",
    "    keys_list = []\n",
    "    for key in earnings_historical:\n",
    "        if key in earning_dict:\n",
    "            keys_list.append(key)\n",
    "        else:\n",
    "            continue\n",
    "    for key in keys_list:\n",
    "        earnings_historical.pop(key)\n",
    "    earnings_historical.update(earning_dict)\n",
    "    write_json(earnings_path, earnings_historical)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aec48301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell contains all of the overarching paths I need for my functions\n",
    "stock_tuple_path = os.path.join('List_of_Stocks', 'nyse_and_nasdaq_stock_symbol_list.json')\n",
    "bull_path = os.path.join('Market_Trend_Data', 'Bullish_Bearish_Neutral_Data')\n",
    "inf_path = os.path.join('Market_Trend_Data', 'Inflation_Percentage_History')\n",
    "earnings_path = os.path.join('Market_Trend_Data', 'Earnings_Reports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This for loop will loop through all of the stock tuples and save the stock json data to json\n",
    "stock_tuples = obtain_stock_tuple_list(stock_tuple_path)\n",
    "failed_stock_retrievals = []\n",
    "dow_df = obtain_dow()\n",
    "sp_df = obtain_sp()\n",
    "nasdaq_df = obtain_nasdaq()\n",
    "investor_sentiment_df = obtain_investor_sentiment(bull_path)\n",
    "inflation_df = obtain_inflation(inf_path)\n",
    "earnings_dict = obtain_earnings(earnings_path)\n",
    "for stock_tuple in stock_tuples:\n",
    "    check_wifi()\n",
    "    historical_pricing_df = obtain_historical_pricing(stock_tuple)\n",
    "    if len(historical_pricing_df) <= 90:\n",
    "        failed_stock_retrievals.append(stock_tuple)\n",
    "    else:\n",
    "        earnings_df = map_earnings(earnings_dict, historical_pricing_df, stock_tuple[0])\n",
    "        master_df =  merge_dfs(historical_pricing_df, \\\n",
    "                               investor_sentiment_df, inflation_df, dow_df, sp_df, nasdaq_df, earnings_df)\n",
    "        master_json(master_df, stock_tuple)\n",
    "        print('json_created')\n",
    "print(len(failed_stock_retrievals))\n",
    "save_failed_scrapes(failed_stock_retrievals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
